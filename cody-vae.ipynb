{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "030ac1b0",
   "metadata": {},
   "source": [
    "## Variational Autoencoder (VAE) on MNIST (PyTorch)\n",
    "\n",
    "Train a simple VAE to reconstruct and sample MNIST digits. Configure hyperparameters below, then run cells in order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b741ebb-1079-440c-97e8-0f3461265ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import os\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, utils as vutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d8007a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Config(data_dir='/home/ubuntu/.data/mnist', batch_size=128, epochs=5, lr=0.001, latent_dim=20, device='cuda', num_workers=2, seed=42, output_dir='./vae_outputs')\n"
     ]
    }
   ],
   "source": [
    "# Setup and imports\n",
    "import os\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, utils as vutils\n",
    "\n",
    "# Configuration\n",
    "@dataclass\n",
    "class Config:\n",
    "    data_dir: str = os.path.expanduser(\"~/.data/mnist\")\n",
    "    batch_size: int = 128\n",
    "    epochs: int = 5\n",
    "    lr: float = 1e-3\n",
    "    latent_dim: int = 20\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    num_workers: int = 2\n",
    "    seed: int = 42\n",
    "    output_dir: str = \"./vae_outputs\"\n",
    "\n",
    "cfg = Config()\n",
    "os.makedirs(cfg.output_dir, exist_ok=True)\n",
    "\n",
    "torch.manual_seed(cfg.seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(cfg.seed)\n",
    "\n",
    "print(f\"Using device: {cfg.device}\")\n",
    "print(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "20c7055e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 469, Val batches: 79\n"
     ]
    }
   ],
   "source": [
    "# Data: MNIST loaders\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # converts to [0,1]\n",
    "])\n",
    "\n",
    "train_ds = datasets.MNIST(root=cfg.data_dir, train=True, transform=transform, download=True)\n",
    "val_ds = datasets.MNIST(root=cfg.data_dir, train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True, num_workers=cfg.num_workers, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers, pin_memory=True)\n",
    "\n",
    "len_train = len(train_loader)\n",
    "len_val = len(val_loader)\n",
    "print(f\"Train batches: {len_train}, Val batches: {len_val}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9ad4b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE(\n",
      "  (encoder): Encoder(\n",
      "    (net): Sequential(\n",
      "      (0): Flatten(start_dim=1, end_dim=-1)\n",
      "      (1): Linear(in_features=784, out_features=400, bias=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (mu): Linear(in_features=400, out_features=20, bias=True)\n",
      "    (logvar): Linear(in_features=400, out_features=20, bias=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (net): Sequential(\n",
      "      (0): Linear(in_features=20, out_features=400, bias=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=400, out_features=784, bias=True)\n",
      "      (3): Sigmoid()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# VAE model\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim: int):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(28 * 28, 400),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.mu = nn.Linear(400, latent_dim)\n",
    "        self.logvar = nn.Linear(400, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.net(x)\n",
    "        return self.mu(h), self.logvar(h)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim: int):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 400),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(400, 28 * 28),\n",
    "            nn.Sigmoid(),  # output in [0,1]\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        x_hat = self.net(z)\n",
    "        return x_hat.view(-1, 1, 28, 28)\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim: int):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(latent_dim)\n",
    "        self.decoder = Decoder(latent_dim)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat, mu, logvar\n",
    "\n",
    "model = VAE(cfg.latent_dim).to(cfg.device)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e9cbe1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and utilities\n",
    "\n",
    "def vae_loss(x, x_hat, mu, logvar):\n",
    "    # Reconstruction loss: binary cross entropy over pixels\n",
    "    recon = F.binary_cross_entropy(x_hat, x, reduction='sum')\n",
    "    # KL divergence between q(z|x) and p(z) ~ N(0, I)\n",
    "    kld = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return recon + kld, recon, kld\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_recon = 0.0\n",
    "    total_kld = 0.0\n",
    "    total_elems = 0\n",
    "    for x, _ in loader:\n",
    "        x = x.to(device)\n",
    "        x_hat, mu, logvar = model(x)\n",
    "        loss, recon, kld = vae_loss(x, x_hat, mu, logvar)\n",
    "        total_loss += loss.item()\n",
    "        total_recon += recon.item()\n",
    "        total_kld += kld.item()\n",
    "        total_elems += x.size(0)\n",
    "    return {\n",
    "        'loss_per_img': total_loss / total_elems,\n",
    "        'recon_per_img': total_recon / total_elems,\n",
    "        'kld_per_img': total_kld / total_elems,\n",
    "    }\n",
    "\n",
    "def save_image_grid(tensor, path, nrow=8):\n",
    "    vutils.save_image(tensor, path, nrow=nrow, padding=2, normalize=True)\n",
    "    print(f\"Saved: {path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a60a22bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | train loss 163.83 (recon 148.29, kld 15.54) | val loss 126.62\n",
      "Saved: ./vae_outputs/recon_epoch_001.png\n",
      "Saved: ./vae_outputs/samples_epoch_001.png\n",
      "Saved best model.\n",
      "Epoch 002 | train loss 120.86 (recon 98.54, kld 22.33) | val loss 115.44\n",
      "Saved: ./vae_outputs/recon_epoch_002.png\n",
      "Saved: ./vae_outputs/samples_epoch_002.png\n",
      "Saved best model.\n",
      "Epoch 003 | train loss 114.12 (recon 90.16, kld 23.96) | val loss 111.60\n",
      "Saved: ./vae_outputs/recon_epoch_003.png\n",
      "Saved: ./vae_outputs/samples_epoch_003.png\n",
      "Saved best model.\n",
      "Epoch 004 | train loss 111.33 (recon 86.86, kld 24.48) | val loss 109.45\n",
      "Saved: ./vae_outputs/recon_epoch_004.png\n",
      "Saved: ./vae_outputs/samples_epoch_004.png\n",
      "Saved best model.\n",
      "Epoch 005 | train loss 109.67 (recon 84.90, kld 24.77) | val loss 108.33\n",
      "Saved: ./vae_outputs/recon_epoch_005.png\n",
      "Saved: ./vae_outputs/samples_epoch_005.png\n",
      "Saved best model.\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=cfg.lr)\n",
    "\n",
    "fixed_noise = torch.randn(64, cfg.latent_dim, device=cfg.device)\n",
    "\n",
    "best_val = math.inf\n",
    "for epoch in range(1, cfg.epochs + 1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_recon = 0.0\n",
    "    running_kld = 0.0\n",
    "    count = 0\n",
    "\n",
    "    for x, _ in train_loader:\n",
    "        x = x.to(cfg.device)\n",
    "        x_hat, mu, logvar = model(x)\n",
    "        loss, recon, kld = vae_loss(x, x_hat, mu, logvar)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        running_recon += recon.item()\n",
    "        running_kld += kld.item()\n",
    "        count += x.size(0)\n",
    "\n",
    "    train_loss = running_loss / count\n",
    "    train_recon = running_recon / count\n",
    "    train_kld = running_kld / count\n",
    "\n",
    "    val_metrics = evaluate(model, val_loader, cfg.device)\n",
    "\n",
    "    print(f\"Epoch {epoch:03d} | train loss {train_loss:.2f} (recon {train_recon:.2f}, kld {train_kld:.2f}) | \"\n",
    "          f\"val loss {val_metrics['loss_per_img']:.2f}\")\n",
    "\n",
    "    # Reconstructions\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x_sample, _ = next(iter(val_loader))\n",
    "        x_sample = x_sample[:64].to(cfg.device)\n",
    "        x_hat, _, _ = model(x_sample)\n",
    "        grid = torch.cat([x_sample, x_hat], dim=0)\n",
    "        save_image_grid(grid, os.path.join(cfg.output_dir, f\"recon_epoch_{epoch:03d}.png\"), nrow=8)\n",
    "\n",
    "        # Random samples from prior\n",
    "        samples = model.decoder(fixed_noise)\n",
    "        save_image_grid(samples, os.path.join(cfg.output_dir, f\"samples_epoch_{epoch:03d}.png\"), nrow=8)\n",
    "\n",
    "    # Track best\n",
    "    if val_metrics['loss_per_img'] < best_val:\n",
    "        best_val = val_metrics['loss_per_img']\n",
    "        torch.save(model.state_dict(), os.path.join(cfg.output_dir, \"vae_best.pt\"))\n",
    "        print(\"Saved best model.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b8c0f7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated config for convergence: Config(data_dir='/home/ubuntu/.data/mnist', batch_size=128, epochs=20, lr=0.001, latent_dim=20, device='cuda', num_workers=2, seed=42, output_dir='./vae_outputs')\n"
     ]
    }
   ],
   "source": [
    "# Convergence settings (KL warmup, early stopping, scheduler)\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Increase epochs if needed\n",
    "cfg.epochs = max(cfg.epochs, 20)\n",
    "cfg.patience = 5\n",
    "cfg.min_delta = 1e-3\n",
    "cfg.kl_warmup_epochs = 10\n",
    "cfg.beta_start = 0.0\n",
    "cfg.beta_end = 1.0\n",
    "cfg.min_lr = 1e-5\n",
    "\n",
    "print(\"Updated config for convergence:\", cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6762de0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Override loss with KL warmup and helpers\n",
    "\n",
    "def compute_beta(epoch: int, step: int, steps_per_epoch: int) -> float:\n",
    "    total_warmup_steps = max(1, cfg.kl_warmup_epochs * steps_per_epoch)\n",
    "    current_step = (epoch - 1) * steps_per_epoch + step\n",
    "    t = min(1.0, current_step / total_warmup_steps)\n",
    "    return float(cfg.beta_start + t * (cfg.beta_end - cfg.beta_start))\n",
    "\n",
    "\n",
    "def vae_loss(x, x_hat, mu, logvar, beta: float = 1.0):\n",
    "    recon = F.binary_cross_entropy(x_hat, x, reduction='sum')\n",
    "    kld = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return recon + beta * kld, recon, kld\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device, beta: float = 1.0):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_recon = 0.0\n",
    "    total_kld = 0.0\n",
    "    total_elems = 0\n",
    "    for x, _ in loader:\n",
    "        x = x.to(device)\n",
    "        x_hat, mu, logvar = model(x)\n",
    "        loss, recon, kld = vae_loss(x, x_hat, mu, logvar, beta=beta)\n",
    "        total_loss += loss.item()\n",
    "        total_recon += recon.item()\n",
    "        total_kld += kld.item()\n",
    "        total_elems += x.size(0)\n",
    "    return {\n",
    "        'loss_per_img': total_loss / total_elems,\n",
    "        'recon_per_img': total_recon / total_elems,\n",
    "        'kld_per_img': total_kld / total_elems,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238dd6f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | beta ~ 0.00->0.10 | train 71.95 (recon 69.11, kld 60.45) | val 120.71 | lr 1.00e-03\n",
      "Saved: ./vae_outputs/recon_epoch_001.png\n",
      "Saved: ./vae_outputs/samples_epoch_001.png\n",
      "Saved: ./vae_outputs/vae_last.pt\n",
      "Saved best model.\n",
      "Epoch 002 | beta ~ 0.10->0.20 | train 76.61 (recon 69.48, kld 47.92) | val 114.45 | lr 1.00e-03\n",
      "Saved: ./vae_outputs/recon_epoch_002.png\n",
      "Saved: ./vae_outputs/samples_epoch_002.png\n",
      "Saved: ./vae_outputs/vae_last.pt\n",
      "Saved best model.\n",
      "Epoch 003 | beta ~ 0.20->0.30 | train 80.78 (recon 70.23, kld 42.31) | val 110.91 | lr 1.00e-03\n",
      "Saved: ./vae_outputs/recon_epoch_003.png\n",
      "Saved: ./vae_outputs/samples_epoch_003.png\n",
      "Saved: ./vae_outputs/vae_last.pt\n",
      "Saved best model.\n",
      "Epoch 004 | beta ~ 0.30->0.40 | train 84.60 (recon 71.12, kld 38.58) | val 108.12 | lr 1.00e-03\n",
      "Saved: ./vae_outputs/recon_epoch_004.png\n",
      "Saved: ./vae_outputs/samples_epoch_004.png\n",
      "Saved: ./vae_outputs/vae_last.pt\n",
      "Saved best model.\n",
      "Epoch 005 | beta ~ 0.40->0.50 | train 88.13 (recon 72.07, kld 35.74) | val 107.45 | lr 1.00e-03\n",
      "Saved: ./vae_outputs/recon_epoch_005.png\n",
      "Saved: ./vae_outputs/samples_epoch_005.png\n",
      "Saved: ./vae_outputs/vae_last.pt\n",
      "Saved best model.\n",
      "Epoch 006 | beta ~ 0.50->0.60 | train 91.45 (recon 73.06, kld 33.46) | val 106.08 | lr 1.00e-03\n",
      "Saved: ./vae_outputs/recon_epoch_006.png\n",
      "Saved: ./vae_outputs/samples_epoch_006.png\n",
      "Saved: ./vae_outputs/vae_last.pt\n",
      "Saved best model.\n",
      "Epoch 007 | beta ~ 0.60->0.70 | train 94.58 (recon 74.06, kld 31.59) | val 105.35 | lr 1.00e-03\n",
      "Saved: ./vae_outputs/recon_epoch_007.png\n",
      "Saved: ./vae_outputs/samples_epoch_007.png\n",
      "Saved: ./vae_outputs/vae_last.pt\n",
      "Saved best model.\n",
      "Epoch 008 | beta ~ 0.70->0.80 | train 97.54 (recon 75.09, kld 29.94) | val 104.82 | lr 1.00e-03\n",
      "Saved: ./vae_outputs/recon_epoch_008.png\n",
      "Saved: ./vae_outputs/samples_epoch_008.png\n",
      "Saved: ./vae_outputs/vae_last.pt\n",
      "Saved best model.\n",
      "Epoch 009 | beta ~ 0.80->0.90 | train 100.31 (recon 76.10, kld 28.50) | val 104.52 | lr 1.00e-03\n",
      "Saved: ./vae_outputs/recon_epoch_009.png\n",
      "Saved: ./vae_outputs/samples_epoch_009.png\n",
      "Saved: ./vae_outputs/vae_last.pt\n",
      "Saved best model.\n",
      "Epoch 010 | beta ~ 0.90->1.00 | train 103.03 (recon 77.21, kld 27.19) | val 104.37 | lr 1.00e-03\n",
      "Saved: ./vae_outputs/recon_epoch_010.png\n",
      "Saved: ./vae_outputs/samples_epoch_010.png\n",
      "Saved: ./vae_outputs/vae_last.pt\n",
      "Saved best model.\n",
      "Epoch 011 | beta ~ 1.00->1.00 | train 104.22 (recon 77.75, kld 26.47) | val 103.99 | lr 1.00e-03\n",
      "Saved: ./vae_outputs/recon_epoch_011.png\n",
      "Saved: ./vae_outputs/samples_epoch_011.png\n",
      "Saved: ./vae_outputs/vae_last.pt\n",
      "Saved best model.\n",
      "Epoch 012 | beta ~ 1.00->1.00 | train 104.06 (recon 77.73, kld 26.33) | val 104.17 | lr 1.00e-03\n",
      "Saved: ./vae_outputs/recon_epoch_012.png\n",
      "Saved: ./vae_outputs/samples_epoch_012.png\n",
      "Saved: ./vae_outputs/vae_last.pt\n",
      "Epoch 013 | beta ~ 1.00->1.00 | train 103.94 (recon 77.69, kld 26.24) | val 103.86 | lr 1.00e-03\n",
      "Saved: ./vae_outputs/recon_epoch_013.png\n",
      "Saved: ./vae_outputs/samples_epoch_013.png\n",
      "Saved: ./vae_outputs/vae_last.pt\n",
      "Saved best model.\n",
      "Epoch 014 | beta ~ 1.00->1.00 | train 103.86 (recon 77.64, kld 26.22) | val 103.69 | lr 1.00e-03\n",
      "Saved: ./vae_outputs/recon_epoch_014.png\n",
      "Saved: ./vae_outputs/samples_epoch_014.png\n",
      "Saved: ./vae_outputs/vae_last.pt\n",
      "Saved best model.\n",
      "Epoch 015 | beta ~ 1.00->1.00 | train 103.73 (recon 77.58, kld 26.14) | val 103.60 | lr 1.00e-03\n",
      "Saved: ./vae_outputs/recon_epoch_015.png\n",
      "Saved: ./vae_outputs/samples_epoch_015.png\n",
      "Saved: ./vae_outputs/vae_last.pt\n",
      "Saved best model.\n",
      "Epoch 016 | beta ~ 1.00->1.00 | train 103.62 (recon 77.49, kld 26.13) | val 103.62 | lr 1.00e-03\n",
      "Saved: ./vae_outputs/recon_epoch_016.png\n",
      "Saved: ./vae_outputs/samples_epoch_016.png\n",
      "Saved: ./vae_outputs/vae_last.pt\n",
      "Epoch 017 | beta ~ 1.00->1.00 | train 103.54 (recon 77.45, kld 26.10) | val 103.60 | lr 5.00e-04\n",
      "Saved: ./vae_outputs/recon_epoch_017.png\n",
      "Saved: ./vae_outputs/samples_epoch_017.png\n",
      "Saved: ./vae_outputs/vae_last.pt\n",
      "Epoch 018 | beta ~ 1.00->1.00 | train 102.80 (recon 76.72, kld 26.08) | val 102.67 | lr 5.00e-04\n",
      "Saved: ./vae_outputs/recon_epoch_018.png\n",
      "Saved: ./vae_outputs/samples_epoch_018.png\n",
      "Saved: ./vae_outputs/vae_last.pt\n",
      "Saved best model.\n"
     ]
    }
   ],
   "source": [
    "# Training loop with KL warmup, early stopping, LR scheduling, and robust checkpointing\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=cfg.lr)\n",
    "scheduler = ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    factor=0.5,\n",
    "    patience=2,\n",
    "    threshold=cfg.min_delta,\n",
    "    min_lr=cfg.min_lr,\n",
    "    # verbose argument removed to fix TypeError\n",
    ")\n",
    "\n",
    "fixed_noise = torch.randn(64, cfg.latent_dim, device=cfg.device)\n",
    "\n",
    "best_val = math.inf\n",
    "epochs_no_improve = 0\n",
    "\n",
    "\n",
    "def save_checkpoint(path: str):\n",
    "    torch.save({'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'cfg': cfg.__dict__}, path)\n",
    "    print(f\"Saved: {path}\")\n",
    "\n",
    "\n",
    "try:\n",
    "    for epoch in range(1, cfg.epochs + 1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_recon = 0.0\n",
    "        running_kld = 0.0\n",
    "        count = 0\n",
    "\n",
    "        for step, (x, _) in enumerate(train_loader, start=1):\n",
    "            x = x.to(cfg.device)\n",
    "            x_hat, mu, logvar = model(x)\n",
    "            beta = compute_beta(epoch, step, len_train)\n",
    "            loss, recon, kld = vae_loss(x, x_hat, mu, logvar, beta=beta)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            running_recon += recon.item()\n",
    "            running_kld += kld.item()\n",
    "            count += x.size(0)\n",
    "\n",
    "        train_loss = running_loss / count\n",
    "        train_recon = running_recon / count\n",
    "        train_kld = running_kld / count\n",
    "\n",
    "        val_metrics = evaluate(model, val_loader, cfg.device, beta=cfg.beta_end)\n",
    "        scheduler.step(val_metrics['loss_per_img'])\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch:03d} | beta ~ {compute_beta(epoch, 1, len_train):.2f}->{compute_beta(epoch, len_train, len_train):.2f} | \"\n",
    "            f\"train {train_loss:.2f} (recon {train_recon:.2f}, kld {train_kld:.2f}) | \"\n",
    "            f\"val {val_metrics['loss_per_img']:.2f} | lr {optimizer.param_groups[0]['lr']:.2e}\"\n",
    "        )\n",
    "\n",
    "        # Reconstructions and samples\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            x_sample, _ = next(iter(val_loader))\n",
    "            x_sample = x_sample[:64].to(cfg.device)\n",
    "            x_hat, _, _ = model(x_sample)\n",
    "            grid = torch.cat([x_sample, x_hat], dim=0)\n",
    "            save_image_grid(grid, os.path.join(cfg.output_dir, f\"recon_epoch_{epoch:03d}.png\"), nrow=8)\n",
    "\n",
    "            samples = model.decoder(fixed_noise)\n",
    "            save_image_grid(samples, os.path.join(cfg.output_dir, f\"samples_epoch_{epoch:03d}.png\"), nrow=8)\n",
    "\n",
    "        # Save last checkpoint each epoch\n",
    "        save_checkpoint(os.path.join(cfg.output_dir, \"vae_last.pt\"))\n",
    "\n",
    "        # Early stopping and best model\n",
    "        if val_metrics['loss_per_img'] + cfg.min_delta < best_val:\n",
    "            best_val = val_metrics['loss_per_img']\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), os.path.join(cfg.output_dir, \"vae_best.pt\"))\n",
    "            print(\"Saved best model.\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= cfg.patience:\n",
    "                print(f\"Early stopping triggered after no improvement for {cfg.patience} epochs.\")\n",
    "                break\n",
    "\n",
    "except RuntimeError as e:\n",
    "    if \"interrupted by user\" in str(e).lower():\n",
    "        print(\"Training interrupted by Jupyter kernel interrupt.\")\n",
    "    else:\n",
    "        raise\n",
    "finally:\n",
    "    # Ensure last weights are saved even if interrupted\n",
    "    torch.save(model.state_dict(), os.path.join(cfg.output_dir, \"vae_last_weights_only.pt\"))\n",
    "    save_checkpoint(os.path.join(cfg.output_dir, \"vae_last.pt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fd9062-e76f-46d8-b2cb-6ae63c8886ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5bbb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "@torch.no_grad()\n",
    "def show_grid(img_path, title=None):\n",
    "    img = plt.imread(img_path)\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# Display last saved grids if they exist\n",
    "recon_path = os.path.join(cfg.output_dir, f\"recon_epoch_{cfg.epochs:03d}.png\")\n",
    "samples_path = os.path.join(cfg.output_dir, f\"samples_epoch_{cfg.epochs:03d}.png\")\n",
    "if os.path.exists(recon_path):\n",
    "    show_grid(recon_path, title=\"Reconstructions (top: input, bottom: reconstruction)\")\n",
    "if os.path.exists(samples_path):\n",
    "    show_grid(samples_path, title=\"Random samples from N(0, I)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "44bf383c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. Number of devices: 1\n",
      "Current device: 0\n",
      "Device name: NVIDIA A100-SXM4-40GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA is available. Number of devices: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current device: {torch.cuda.current_device()}\")\n",
    "    print(f\"Device name: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a568e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
